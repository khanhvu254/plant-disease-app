import os
os.environ["QT_QPA_PLATFORM"] = "offscreen"
os.environ["OPENCV_VIDEOIO_PRIORITY_MSMF"] = "0"

import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import pandas as pd
import torch
from torchvision import transforms
from torchvision.models import resnet50, ResNet50_Weights
from ultralytics import YOLO
import streamlit as st

# ====== C·∫•u h√¨nh trang ======
st.set_page_config(
    page_title="Nh·∫≠n D·∫°ng B·ªánh C√¢y Tr·ªìng",
    page_icon="üåø",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ====== Custom CSS ======
st.markdown("""
    <style>
    .main {
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    }
    .stTitle {
        color: #2d5016;
        text-align: center;
        font-size: 3rem !important;
        font-weight: bold;
        padding: 1rem 0;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .upload-section {
        background: white;
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    .result-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        box-shadow: 0 8px 16px rgba(0,0,0,0.2);
        margin: 1rem 0;
    }
    .metric-box {
        background: rgba(255,255,255,0.2);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        backdrop-filter: blur(10px);
    }
    .info-box {
        background: #e8f5e9;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #4caf50;
        margin: 1rem 0;
    }
    .stButton>button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        padding: 0.75rem 2rem;
        border-radius: 25px;
        font-weight: bold;
        width: 100%;
        transition: all 0.3s;
    }
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.2);
    }
    .detection-box {
        background: #fff3cd;
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #ffc107;
        margin: 1rem 0;
    }
    </style>
""", unsafe_allow_html=True)


# ====== Load models ======
@st.cache_resource
def load_models():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load  model
    MODEL_PATH = 'model.pth'
    CSV_PATH = 'dataset_labels.csv'

    # ƒê·ªçc danh s√°ch nh√£n
    df = pd.read_csv(CSV_PATH)
    class_names = sorted(df["label"].unique().tolist())

    # Kh·ªüi t·∫°o  model
    num_classes = len(class_names)
    model = resnet50(weights=ResNet50_Weights.DEFAULT)
    in_features = model.fc.in_features
    model.fc = torch.nn.Linear(in_features, num_classes)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()

    # Load YOLO model
    YOLO_MODEL_PATH = 'yolo11n.pt'
    yolo_model = YOLO(YOLO_MODEL_PATH)

    return model, yolo_model, class_names, DEVICE


def draw_bounding_boxes(image, detections):
    """V·∫Ω bounding box l√™n ·∫£nh"""
    img_draw = image.copy()
    draw = ImageDraw.Draw(img_draw)

    # T·∫°o font (n·∫øu kh√¥ng c√≥ font, s·∫Ω d√πng font m·∫∑c ƒë·ªãnh)
    try:
        font = ImageFont.truetype("arial.ttf", 20)
    except:
        font = ImageFont.load_default()

    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]

    for idx, det in enumerate(detections):
        box = det['box']
        label = det['label']

        color = colors[idx % len(colors)]

        # V·∫Ω bounding box
        draw.rectangle(box, outline=color, width=3)

        # V·∫Ω label (b·ªè confidence)
        text = f"{label}"

        # V·∫Ω background cho text
        bbox = draw.textbbox((box[0], box[1] - 25), text, font=font)
        draw.rectangle(bbox, fill=color)
        draw.text((box[0], box[1] - 25), text, fill="white", font=font)

    return img_draw


def detect_with_yolo(yolo_model, image):
    """Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi YOLO"""
    # Convert PIL to OpenCV format
    img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    # Ch·∫°y YOLO detection
    results = yolo_model(img_cv, conf=0.25)  # confidence threshold 0.25

    detections = []

    for result in results:
        boxes = result.boxes
        for box in boxes:
            # L·∫•y t·ªça ƒë·ªô bounding box
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()

            # L·∫•y confidence v√† class
            confidence = float(box.conf[0].cpu().numpy())
            cls = int(box.cls[0].cpu().numpy())
            label = result.names[cls]

            detections.append({
                'box': (int(x1), int(y1), int(x2), int(y2)),
                'label': label,
                'confidence': confidence,
                'class_id': cls
            })

    return detections


def crop_detection_for_classification(image, box):
    """C·∫Øt v√πng detection ƒë·ªÉ ph√¢n lo·∫°i"""
    x1, y1, x2, y2 = box
    return image.crop((x1, y1, x2, y2))


# Ti·ªÅn x·ª≠ l√Ω ·∫£nh
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# ====== Sidebar ======
with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/628/628283.png", width=150)
    st.title("üìã H∆∞·ªõng D·∫´n")

    # Ch·ªçn ch·∫ø ƒë·ªô
    detection_mode = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô ph√°t hi·ªán:",
        ["YOLO + Classification", "Classification Only"],
        help="YOLO s·∫Ω ph√°t hi·ªán v·ªã tr√≠ b·ªánh, sau ƒë√≥ ph√¢n lo·∫°i chi ti·∫øt"
    )

    st.markdown("""
    ### C√°ch s·ª≠ d·ª•ng:
    1. üì§ T·∫£i l√™n ·∫£nh c√¢y tr·ªìng
    2. üéØ Ch·ªçn ch·∫ø ƒë·ªô ph√°t hi·ªán
    3. ‚è≥ ƒê·ª£i h·ªá th·ªëng ph√¢n t√≠ch
    4. üìä Xem k·∫øt qu·∫£ d·ª± ƒëo√°n

    ### ƒê·ªãnh d·∫°ng ·∫£nh:
    - JPG, PNG, JPEG
    - Ch·∫•t l∆∞·ª£ng t·ªët
    - R√µ n√©t, ƒë·ªß √°nh s√°ng

    ### L∆∞u √Ω:
    ‚ö†Ô∏è K·∫øt qu·∫£ ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o
    """)

    st.markdown("---")
    st.markdown("### üîß Th√¥ng Tin H·ªá Th·ªëng")
    device_type = "GPU (CUDA)" if torch.cuda.is_available() else "CPU"
    st.info(f"**Thi·∫øt b·ªã:** {device_type}")
    st.info(f"**Ch·∫ø ƒë·ªô:** {detection_mode}")

# ====== Main Content ======
st.title("üåø H·ªÜ TH·ªêNG NH·∫¨N D·∫†NG B·ªÜNH C√ÇY TR·ªíNG")
st.markdown(
    "<p style='text-align: center; color: #666; font-size: 1.2rem;'>S·ª≠ d·ª•ng AI (YOLO + Faster-RCNN) ƒë·ªÉ ph√°t hi·ªán v√† ch·∫©n ƒëo√°n b·ªánh tr√™n c√¢y tr·ªìng</p>",
    unsafe_allow_html=True)

# Load models
try:
    model, yolo_model, class_names, DEVICE = load_models()
except Exception as e:
    st.error(f"‚ùå L·ªói khi t·∫£i model: {str(e)}")
    st.info("üí° L∆∞u √Ω: ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√≥ file 'model' trong th∆∞ m·ª•c d·ª± √°n")
    st.stop()

# Upload section
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    st.markdown("<div class='upload-section'>", unsafe_allow_html=True)
    uploaded_file = st.file_uploader(
        "Ch·ªçn ·∫£nh c√¢y tr·ªìng c·ªßa b·∫°n",
        type=["jpg", "png", "jpeg"],
        help="T·∫£i l√™n ·∫£nh c√¢y tr·ªìng ƒë·ªÉ ph√°t hi·ªán b·ªánh"
    )
    st.markdown("</div>", unsafe_allow_html=True)

# Processing and Results
if uploaded_file is not None:
    image = Image.open(uploaded_file).convert("RGB")

    if detection_mode == "YOLO + Classification":
        # ====== YOLO Detection Mode ======
        st.markdown("### üéØ Ph√°t hi·ªán v·ªõi YOLO + Ph√¢n lo·∫°i v·ªõi Faster-RCNN")

        col_original, col_detected = st.columns(2)

        with col_original:
            st.markdown("#### üì∏ ·∫¢nh G·ªëc")
            st.image(image, use_container_width=True, caption="·∫¢nh b·∫°n ƒë√£ t·∫£i l√™n")

        with col_detected:
            st.markdown("#### üîç ƒêang Ph√¢n T√≠ch...")
            progress_bar = st.progress(0)

            # YOLO Detection
            progress_bar.progress(30)
            detections = detect_with_yolo(yolo_model, image)

            progress_bar.progress(60)

            if len(detections) > 0:
                # V·∫Ω bounding boxes
                img_with_boxes = draw_bounding_boxes(image, detections)
                st.image(img_with_boxes, use_container_width=True,
                         caption=f"Ph√°t hi·ªán {len(detections)} v√πng b·ªánh")
                progress_bar.progress(100)
            else:
                st.warning("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c v√πng b·ªánh n√†o")
                progress_bar.progress(100)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt
        if len(detections) > 0:
            st.markdown("---")
            st.markdown("### üìä K·∫øt Qu·∫£ Chi Ti·∫øt T·ª´ng V√πng")

            for idx, det in enumerate(detections, 1):
                with st.expander(f"üî¨ V√πng {idx}: {det['label']}"):
                    col_crop, col_class = st.columns(2)

                    with col_crop:
                        # Hi·ªÉn th·ªã v√πng ƒë√£ crop
                        cropped_img = crop_detection_for_classification(image, det['box'])
                        st.image(cropped_img, caption=f"V√πng ph√°t hi·ªán {idx}",
                                 use_container_width=True)

                    with col_class:
                        # Ph√¢n lo·∫°i chi ti·∫øt
                        st.markdown("**üß¨ Ph√¢n lo·∫°i chi ti·∫øt:**")

                        img_tensor = transform(cropped_img).unsqueeze(0).to(DEVICE)
                        with torch.no_grad():
                            outputs = model(img_tensor)
                            probs = torch.nn.functional.softmax(outputs, dim=1)
                            pred_idx = torch.argmax(probs, dim=1).item()

                        pred_label = class_names[pred_idx]

                        # Ph√¢n t√≠ch nh√£n
                        if "_" in pred_label:
                            plant, disease = pred_label.split("_", 1)
                        else:
                            plant, disease = pred_label, "Kh√¥ng ph√°t hi·ªán b·ªánh"

                        st.markdown(f"""
                        <div class='detection-box'>
                            <p><b>üå± Lo·∫°i c√¢y:</b> {plant.capitalize()}</p>
                            <p><b>ü¶† B·ªánh:</b> {disease.replace('_', ' ').title()}</p>
                        </div>
                        """, unsafe_allow_html=True)

    else:
        # ====== Classification Only Mode ======
        col_img, col_result = st.columns(2)

        with col_img:
            st.markdown("### üì∏ ·∫¢nh ƒê·∫ßu V√†o")
            st.image(image, use_container_width=True, caption="·∫¢nh b·∫°n ƒë√£ t·∫£i l√™n")

        with col_result:
            st.markdown("### üîç ƒêang Ph√¢n T√≠ch...")

            progress_bar = st.progress(0)
            for i in range(100):
                progress_bar.progress(i + 1)

            # D·ª± ƒëo√°n
            img_tensor = transform(image).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                outputs = model(img_tensor)
                probs = torch.nn.functional.softmax(outputs, dim=1)
                pred_idx = torch.argmax(probs, dim=1).item()

            pred_label = class_names[pred_idx]

            # Ph√¢n t√≠ch nh√£n
            if "_" in pred_label:
                plant, disease = pred_label.split("_", 1)
            else:
                plant, disease = pred_label, "Kh√¥ng ph√°t hi·ªán b·ªánh"

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.markdown("<div class='result-card'>", unsafe_allow_html=True)
            st.markdown("### üéØ K·∫æT QU·∫¢ PH√ÇN T√çCH")

            st.markdown(f"""
            <div class='metric-box'>
                <h3>üå± Lo·∫°i C√¢y: {plant.capitalize()}</h3>
            </div>
            """, unsafe_allow_html=True)

            st.markdown(f"""
            <div class='metric-box'>
                <h3>ü¶† T√¨nh Tr·∫°ng: {disease.replace('_', ' ').title()}</h3>
            </div>
            """, unsafe_allow_html=True)

            st.markdown("</div>", unsafe_allow_html=True)

        # Recommendations
        st.markdown("---")
        st.markdown("### üí° Khuy·∫øn Ngh·ªã")

        st.markdown(f"""
        <div class='info-box'>
            <h4 style='color: #4caf50;'>‚ö° K·∫øt qu·∫£ ph√¢n t√≠ch</h4>
            <p><b>L·ªùi khuy√™n:</b></p>
            <ul>
                <li>Theo d√µi c√¢y tr·ªìng ƒë·ªãnh k·ª≥</li>
                <li>Tham kh·∫£o th√™m √Ω ki·∫øn chuy√™n gia n·∫øu c·∫ßn</li>
                <li>√Åp d·ª•ng bi·ªán ph√°p ph√≤ng tr·ª´ ph√π h·ª£p</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

else:
    # Welcome message
    st.markdown("""
    <div class='info-box'>
        <h3>üëã Ch√†o m·ª´ng ƒë·∫øn v·ªõi h·ªá th·ªëng nh·∫≠n d·∫°ng b·ªánh c√¢y tr·ªìng!</h3>
        <p>H·ªá th·ªëng s·ª≠ d·ª•ng AI ti√™n ti·∫øn:</p>
        <ul>
            <li><b>YOLO</b>: Ph√°t hi·ªán v·ªã tr√≠ b·ªánh v·ªõi bounding box</li>
            <li><b>Faster-RCNN</b>: Ph√¢n lo·∫°i chi ti·∫øt lo·∫°i b·ªánh</li>
        </ul>
        <p><b>H√£y t·∫£i l√™n m·ªôt b·ª©c ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu!</b></p>
    </div>
    """, unsafe_allow_html=True)

    # Example images section
    st.markdown("### üì∏ ·∫¢nh M·∫´u")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.image("https://cdn-icons-png.flaticon.com/512/2917/2917995.png",
                 caption="·∫¢nh r√µ n√©t", use_container_width=True)
    with col2:
        st.image("https://cdn-icons-png.flaticon.com/512/2917/2917994.png",
                 caption="ƒê·ªß √°nh s√°ng", use_container_width=True)
    with col3:
        st.image("https://cdn-icons-png.flaticon.com/512/2917/2917993.png",
                 caption="Ch·ª•p c·∫≠n c·∫£nh", use_container_width=True)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; padding: 1rem;'>
    <p>üåø Ph√°t tri·ªÉn v·ªõi ‚ù§Ô∏è b·ªüi Nh√≥m 9</p>
    <p style='font-size: 0.9rem;'>Powered by YOLO, PyTorch & Streamlit</p>
</div>
""", unsafe_allow_html=True)